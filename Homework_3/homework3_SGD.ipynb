{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "### Optimization with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(l, grad_l, w0, data, batch_size, n_epochs, alpha=0.01):\n",
    "    \"\"\"\n",
    "        Arguments:\n",
    "            l: the function l(w; D) we want to optimize. It is supposed to be a Python function, not an array.\n",
    "            grad_l: the gradient of l(w; D). It is supposed to be a Python function, not an array.\n",
    "            w0: an n-dimensional array which represents the initial iterate. By default, it should be randomly sampled.\n",
    "            data: a tuple (x, y) that contains the two arrays x and y, where x is the input data, y is the output data.\n",
    "            batch_size: an integer. The dimension of each batch. Should be a divisor of the number of data.\n",
    "            n_epochs: an integer. The number of epochs you want to repeat the iterations.\n",
    "        Returns:\n",
    "            w: an array that contains the value of w_k FOR EACH iterate w_k (not only the latter).\n",
    "            f_val: an array that contains the value of l(w_k; data) FOR EACH iterate w_k ONLY after each epoch.\n",
    "            grads: an array that contains the value of grad_l(w_k; data) FOR EACH iterate w_k ONLY after each epoch.\n",
    "            err: an array the contains the value of ||grad_l(w_k; data)||_2 FOR EACH iterate w_k ONLY after each epoch.\n",
    "    \"\"\"\n",
    "    X, Y = data\n",
    "    d, N = X.shape\n",
    "\n",
    "    # number of epochs * iterations per epoch\n",
    "    tot_iterations = ceil(n_epochs * ceil(N / batch_size))\n",
    "\n",
    "    w = np.zeros(tot_iterations + 1, dtype=object)\n",
    "    f_val = np.zeros(n_epochs, dtype=object)\n",
    "    grads = np.zeros(n_epochs, dtype=object)\n",
    "    err = np.zeros(n_epochs, dtype=object)\n",
    "\n",
    "    w[0] = w0\n",
    "    k = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"--> Epoch {epoch}\")\n",
    "\n",
    "        # Shuffle the dataset\n",
    "        rng = np.random.default_rng()\n",
    "        D = np.concatenate((Y, X.T), axis=1)\n",
    "        rng.shuffle(D)\n",
    "        X = D[:, 1:].T\n",
    "        Y = D[:, 0].reshape(-1, 1)\n",
    "        \n",
    "        batches_num = ceil(N / batch_size)\n",
    "        for batch in range(batches_num):\n",
    "            # print(f\"\\tBatch {batch}\")\n",
    "            Mx = X[:, batch*batch_size:(batch+1)*batch_size]\n",
    "            My = Y[batch*batch_size:(batch+1)*batch_size]\n",
    "            M = (Mx, My)\n",
    "            # Gradient descent step\n",
    "            k = (epoch * batches_num) + batch\n",
    "            w[k+1] = w[k] + alpha * grad_l(w[k], M) \n",
    "\n",
    "        f_val[epoch] = l(w[k+1], data)\n",
    "        grads[epoch] = grad_l(w[k+1], data)\n",
    "        \n",
    "        err[epoch] = np.linalg.norm(grads[epoch])\n",
    "        print(err[epoch])\n",
    "    \n",
    "    return w, f_val, grads, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42000, 784), (42000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load MINST dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/data.csv\")\n",
    "data_raw = np.array(df)\n",
    "X_raw = data_raw[:, 1:]\n",
    "y_raw = data_raw[:, 0]\n",
    "X_raw.shape, y_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8320, 784), (8320,))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_list = [0, 9]\n",
    "\n",
    "mask = [y_i in digit_list for y_i in y_raw]\n",
    "y = y_raw[mask]\n",
    "X = X_raw[mask, :]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 6656), (784, 1664), (6656, 1), (1664, 1))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=False)\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "y_train = y_train.reshape((-1, 1))\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1./(1.+np.exp(-x))\n",
    "\n",
    "def der_sigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l(w, D):\n",
    "    X, y = D\n",
    "    d, N = X.shape\n",
    "    return np.sum(np.power(np.linalg.norm(sigmoid(X.T @ w) - y, axis=1), 2))/N\n",
    "\n",
    "def grad_l(w, D):\n",
    "    X, y = D\n",
    "    d, N = X.shape\n",
    "    der = der_sigmoid(X.T @ w)\n",
    "    res = der * X.T * (sigmoid(X.T @ w) - y)\n",
    "    temp = (2*np.sum(res, axis=0)/N).reshape((-1,1))\n",
    "    return temp\n",
    "\n",
    "def grad_l2(w, D):\n",
    "    X, y = D\n",
    "    d, N = X.shape\n",
    "    def grad_step(arg):\n",
    "        x_i, y_i = (arg[1:], arg[:1])\n",
    "        res_step = der_sigmoid(x_i.T @ w) * x_i.T * (sigmoid(x_i.T @ w) - y_i)\n",
    "        return res_step\n",
    "    full_D = np.concatenate((y.T, X), axis=0)\n",
    "    tmp = np.apply_along_axis(grad_step, 0, full_D)\n",
    "    res = np.sum(tmp, axis=1) / N\n",
    "    return res.reshape((-1,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init weights\n",
    "rng = np.random.default_rng()\n",
    "w0 = rng.normal(size=(X_train.shape[0] + 1, 1))\n",
    "X_hat = np.concatenate((np.ones((1, X_train.shape[1])), X_train), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 1000\n",
    "w, f_val, grads, err = SGD(l, grad_l2, w0, (X_hat, y_train), batch_size, n_epochs, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x for x in range(1, n_epochs + 1)], err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_hat = np.concatenate((np.ones((1, X_test.shape[1])), X_test), axis=0)\n",
    "predictions = sigmoid(X_test_hat.T @ w[-1])\n",
    "y_predicted = np.apply_along_axis(lambda x: 9 if x < 0.5 else 0, 1, predictions).reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_guess):\n",
    "    return (len(y_true) - np.count_nonzero(y_true - y_guess)) / len(y_true) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(y_predicted, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD_log_reg(f, grad_f, w0, D, tolf, tolx, kmax, alpha=0.1):\n",
    "    # As output, we want:\n",
    "    # x -> The stationary point\n",
    "    # f_val -> A vector containing the values of f during the iterations\n",
    "    # err_val -> A vector containing the values of ||grad_f(x_k)||\n",
    "\n",
    "    f_val = [f(w0, D)]\n",
    "    err_val = [grad_f(w0, D)]\n",
    "    \n",
    "    # Init iteration\n",
    "    k = 0\n",
    "    x = [w0]\n",
    "    \n",
    "    rel_tol_cond = True\n",
    "    rel_inp_cond = True\n",
    "    max_it_cond = True\n",
    "    \n",
    "    while rel_tol_cond and rel_inp_cond and max_it_cond:\n",
    "        x.append(x[k] - alpha * grad_f(x[k], D))\n",
    "        f_val.append(f(x[k+1], D))\n",
    "        err_val.append(np.linalg.norm(grad_f(x[k], D)))\n",
    "        \n",
    "        k += 1\n",
    "        if(k % 100 == 0): print(f\"iteration: {k}\")\n",
    "\n",
    "        rel_tol_cond = np.linalg.norm(grad_f(x[k-1], D)) > tolf * np.linalg.norm(grad_f(w0, D))\n",
    "        # We skip the first iteration\n",
    "        rel_inp_cond = True if k == 0 else np.linalg.norm(x[k] - x[k-1]) > tolx\n",
    "        max_it_cond = k < kmax\n",
    "\n",
    "    return x[k], f_val, err_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m9/2qy1g_cd5mjcg_cqty8ydzq00000gn/T/ipykernel_1211/1431357966.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1./(1.+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 100\n",
      "iteration: 200\n",
      "iteration: 300\n",
      "iteration: 400\n",
      "iteration: 500\n"
     ]
    }
   ],
   "source": [
    "res_gd = GD_log_reg(l, grad_l2, w0, (X_hat, y_train), 0., 0., 500, alpha=0.4)\n",
    "w_k, f_val, err_val = res_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.54086538461539"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_hat = np.concatenate((np.ones((1, X_test.shape[1])), X_test), axis=0)\n",
    "predictions = sigmoid(X_test_hat.T @ w_k)\n",
    "y_predicted = np.apply_along_axis(lambda x: 9 if x < 0.5 else 0, 1, predictions).reshape((-1,1))\n",
    "compute_accuracy(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0: 1637\n",
      "class 9: 27\n"
     ]
    }
   ],
   "source": [
    "print(f\"class 0: {y_predicted[y_predicted == 0].shape[0]}\\nclass 9: {y_predicted[y_predicted == 9].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
